# Architecture & Design Decisions

This document explains the key technical decisions, trade-offs, and rationale behind the IOL AI Interviewer platform architecture.

---

## Table of Contents

1. [Technology Stack](#technology-stack)
2. [Assessment Framework](#assessment-framework)
3. [Audio Pipeline](#audio-pipeline)
4. [Frontend Architecture](#frontend-architecture)
5. [Database Design](#database-design)
6. [LLM Integration](#llm-integration)
7. [Scoring System](#scoring-system)
8. [Template Structure](#template-structure)
9. [Alternative Approaches](#alternative-approaches)

---

## Technology Stack

### Backend: FastAPI

**Decision:** Use FastAPI as the backend framework.

**Rationale:**
- **Performance**: Async/await support for handling concurrent interviews
- **Type Safety**: Built-in Pydantic validation reduces runtime errors
- **Documentation**: Auto-generated OpenAPI/Swagger docs at `/docs`
- **Modern**: Native Python 3.12 type hints and async support
- **WebSocket Support**: Built-in WebSocket for real-time audio streaming

**Trade-offs:**
- ‚úÖ **Pros**: Fast, modern, excellent documentation, type-safe
- ‚ö†Ô∏è **Cons**: Smaller ecosystem than Flask/Django, steeper learning curve
- ‚ùå **Alternative Rejected**: Django (too heavyweight, slower async support)

### Database: PostgreSQL

**Decision:** Use PostgreSQL as the primary database.

**Rationale:**
- **JSONB Support**: Store conversation history and assessment data efficiently
- **Reliability**: ACID compliance ensures data integrity
- **Performance**: Connection pooling with SQLAlchemy for scalability
- **Production-Ready**: Battle-tested in enterprise environments

**Trade-offs:**
- ‚úÖ **Pros**: Robust, JSONB for flexible schema, excellent JSON querying
- ‚ö†Ô∏è **Cons**: Requires separate service (not embedded)
- ‚ùå **Alternative Rejected**: SQLite (no concurrency support for production)
- ‚ùå **Alternative Rejected**: MongoDB (less structured, overkill for relational data)

### Cache: Redis

**Decision:** Use Redis for caching and session management.

**Rationale:**
- **Speed**: In-memory cache for interview state
- **Pub/Sub**: Support for real-time updates (future feature)
- **Simplicity**: Simple key-value store for session data
- **TTL Support**: Automatic cleanup of expired sessions

**Trade-offs:**
- ‚úÖ **Pros**: Extremely fast, built-in TTL, pub/sub capability
- ‚ö†Ô∏è **Cons**: Additional service to manage
- ‚ùå **Alternative Rejected**: In-memory Python dict (lost on restart)

---

## Assessment Framework

### 6-Dimension Scoring Model

**Decision:** Implement a 6-dimension weighted scoring system.

**Dimensions & Weights:**
1. **Technical Knowledge** (25%) - Core competency assessment
2. **Problem Solving** (25%) - Critical thinking and approach
3. **Code Quality** (20%) - Standards and best practices
4. **System Design** (10%) - Architecture understanding
5. **Communication** (10%) - Clarity and articulation
6. **Cultural Fit** (10%) - Team alignment

**Rationale:**
- **Balanced**: Higher weights on technical skills (70%), reasonable focus on soft skills (30%)
- **Granular**: 6 dimensions provide detailed feedback vs. single score
- **Evidence-Based**: Each score backed by specific conversation excerpts
- **Industry Standard**: Aligns with typical technical interview rubrics

**Trade-offs:**
- ‚úÖ **Pros**: Detailed feedback, actionable insights, fair assessment
- ‚ö†Ô∏è **Cons**: More complex than single score, requires careful prompt engineering
- ‚ùå **Alternative Rejected**: 3-dimension model (too broad, less actionable)
- ‚ùå **Alternative Rejected**: 10+ dimensions (diminishing returns, harder to differentiate)

### Scoring Scale: 1-5

**Decision:** Use a 5-point scale (Poor, Fair, Good, Very Good, Excellent).

**Rationale:**
- **Simplicity**: Easy to understand and explain
- **Resolution**: 5 points provide sufficient granularity without overwhelming
- **Standard**: Matches common performance review scales
- **LLM-Friendly**: GPT models perform well with 5-point scales

**Trade-offs:**
- ‚úÖ **Pros**: Clear definitions, good granularity, LLM-compatible
- ‚ö†Ô∏è **Cons**: Less granular than 1-10 scale
- ‚ùå **Alternative Rejected**: 1-10 scale (harder to define levels consistently)
- ‚ùå **Alternative Rejected**: Pass/Fail (too coarse, no nuance)

---

## Audio Pipeline

### Multi-Engine TTS Strategy

**Decision:** Implement cascading TTS with 3 engines: gTTS ‚Üí edge-tts ‚Üí pyttsx3.

**Rationale:**
- **Reliability**: Fallback chain ensures audio always works
- **Quality**: gTTS (primary) offers best cloud quality
- **Offline Support**: pyttsx3 (fallback) works without internet
- **Cost**: Free alternatives to paid TTS APIs

**Architecture:**
```
User Question ‚Üí Primary: gTTS (Google Cloud)
                ‚Üì (if fails)
                Secondary: edge-tts (Microsoft Edge)
                ‚Üì (if fails)
                Fallback: pyttsx3 (Offline)
```

**Trade-offs:**
- ‚úÖ **Pros**: High reliability (99.9%+), no single point of failure, cost-effective
- ‚ö†Ô∏è **Cons**: Inconsistent voice across engines, complex error handling
- ‚ùå **Alternative Rejected**: Single paid API (OpenAI TTS) - cost prohibitive for demo
- ‚ùå **Alternative Rejected**: Offline-only - poor voice quality

### Speech Recognition: OpenAI Whisper

**Decision:** Use OpenAI Whisper (via API) for speech-to-text.

**Rationale:**
- **Accuracy**: State-of-the-art transcription (95%+ WER)
- **Robustness**: Handles accents, background noise well
- **Fast**: Cloud-based, low latency (<2 seconds)
- **Cost**: $0.006/minute (affordable for interviews)

**Trade-offs:**
- ‚úÖ **Pros**: Excellent accuracy, fast, handles accents
- ‚ö†Ô∏è **Cons**: Requires internet, costs money (though minimal)
- ‚ùå **Alternative Rejected**: Google Speech-to-Text - similar cost, worse accuracy
- ‚ùå **Alternative Rejected**: Local Whisper model - slow (10-30s), requires GPU

### Audio Format: WAV + MP3

**Decision:** Accept WAV input, store MP3 output.

**Rationale:**
- **WAV Input**: Lossless from browser, no compression artifacts
- **MP3 Output**: Compressed for storage (10:1 ratio), universally supported
- **Processing**: ffmpeg for conversion, scipy for manipulation

**Trade-offs:**
- ‚úÖ **Pros**: High quality input, efficient storage
- ‚ö†Ô∏è **Cons**: Requires ffmpeg dependency
- ‚ùå **Alternative Rejected**: WAV-only - 10x storage cost

---

## Frontend Architecture

### Dual Frontend Approach

**Decision:** Use Streamlit for portals + HTML/JS for voice interview.

**Components:**
1. **Admin Portal** (Streamlit) - Job/interview management
2. **Candidate Portal** (Streamlit) - Resume upload, interview start
3. **Voice Interview** (HTML/JS) - Real-time audio interaction

**Rationale:**
- **Streamlit Strengths**: Rapid development, data visualization, admin CRUD operations
- **HTML/JS Necessity**: WebSocket audio streaming, real-time transcription, browser API access
- **Separation of Concerns**: Admin tasks vs. real-time interaction

**Trade-offs:**
- ‚úÖ **Pros**: Fast admin UI development, high-quality audio interface
- ‚ö†Ô∏è **Cons**: Two different tech stacks to maintain
- ‚ùå **Alternative Rejected**: Full React app - overkill, slower development
- ‚ùå **Alternative Rejected**: Streamlit-only - poor audio/WebSocket support

### Why Not Full SPA?

**Rejected:** Single-page React/Vue application for entire UI.

**Reasons:**
1. **Time Constraint**: Streamlit provides instant admin UI
2. **Complexity**: React requires build pipeline, state management
3. **Audio Requirements**: HTML5 MediaRecorder needed anyway
4. **Maintenance**: Simpler tech stack for future updates

---

## Database Design

### JSONB for Flexible Data

**Decision:** Store conversation history and assessments as JSONB.

**Schema:**
```sql
interviews (
  id: integer
  conversation_history: jsonb  -- Array of {role, content, timestamp}
  assessment_details: jsonb    -- {dimension_scores, evidence, quotes}
  report: jsonb                -- Complete report structure
)
```

**Rationale:**
- **Flexibility**: Schema can evolve without migrations
- **Performance**: PostgreSQL JSONB is indexed and queryable
- **Natural Fit**: LLM responses are already JSON
- **Simplicity**: No separate tables for conversations/scores

**Trade-offs:**
- ‚úÖ **Pros**: Flexible, fast querying, easy to extend
- ‚ö†Ô∏è **Cons**: Less type-safe than normalized tables
- ‚ùå **Alternative Rejected**: Normalized schema - complex joins, slower queries

### Timestamps with Timezone

**Decision:** Use `TIMESTAMP WITH TIME ZONE` for all datetime fields.

**Rationale:**
- **Correctness**: Handles global users across timezones
- **Consistency**: All times stored in UTC
- **Conversion**: Automatic timezone conversion in queries

---

## LLM Integration

### Model Choice: GPT-4o-mini

**Decision:** Use OpenAI GPT-4o-mini as the primary LLM.

**Rationale:**
- **Cost**: 100x cheaper than GPT-4 ($0.15/1M tokens vs $15/1M)
- **Performance**: Sufficient for interview conversations (not code generation)
- **Speed**: Faster responses (1-2s vs 3-5s for GPT-4)
- **Availability**: High rate limits, reliable uptime

**Cost Analysis (per interview):**
- Average tokens: ~3,000 input + 2,000 output = 5,000 total
- Cost: $0.15/1M √ó 5,000 = $0.00075 per interview
- GPT-4 equivalent: $0.075 per interview (100x more)

**Trade-offs:**
- ‚úÖ **Pros**: Cost-effective, fast, good enough for interviews
- ‚ö†Ô∏è **Cons**: Less nuanced than GPT-4, may miss subtle cues
- ‚ùå **Alternative Rejected**: GPT-4 - cost prohibitive for scale
- ‚ùå **Alternative Rejected**: Open-source LLMs - inconsistent quality, complex hosting

### Prompt Engineering Strategy

**Decision:** Use structured prompts with YAML templates.

**Structure:**
```yaml
system_prompt: |
  You are an expert technical interviewer...
  
dimensions:
  - name: Technical Knowledge
    weight: 0.25
    criteria: [...]
    
questions:
  - Easy: [...]
  - Medium: [...]
  - Hard: [...]
```

**Rationale:**
- **Consistency**: Same evaluation criteria across interviews
- **Maintainability**: Easy to update rubrics
- **Version Control**: YAML templates tracked in Git
- **Customization**: Different templates for different roles

---

## Scoring System

### Weighted Average with Evidence

**Decision:** Calculate weighted scores with mandatory evidence.

**Algorithm:**
```python
overall_score = Œ£(dimension_score √ó weight)
confidence = count(evidence) / expected_evidence_count
recommendation = f(overall_score, confidence)
```

**Rationale:**
- **Fairness**: Weights reflect role requirements
- **Transparency**: Evidence shows scoring justification
- **Confidence**: Tracks assessment quality

### Four-Tier Recommendations

**Decision:** Use 4 recommendation levels vs. binary hire/no-hire.

**Levels:**
1. **Strong Hire** (4.0-5.0) - Exceptional candidate
2. **Hire** (3.0-3.9) - Solid candidate
3. **No Hire** (2.0-2.9) - Below bar
4. **Strong No Hire** (1.0-1.9) - Significant concerns

**Rationale:**
- **Nuance**: Differentiates "good" from "exceptional"
- **Actionable**: "Strong Hire" signals urgency
- **Standard**: Matches industry hiring committee practices

**Trade-offs:**
- ‚úÖ **Pros**: More information, clearer signals
- ‚ö†Ô∏è **Cons**: More complex than binary, requires calibration
- ‚ùå **Alternative Rejected**: 5+ levels - too granular, inconsistent

---

## Template Structure

### Single Production Template

**Decision:** Ship with one polished template (Backend Engineer) vs. multiple incomplete ones.

**Rationale:**
- **Quality**: One excellent template > multiple mediocre ones
- **Demo**: Complete backend template showcases all features
- **Extensibility**: Template serves as reference for creating others
- **Scope**: 6 dimensions √ó 5 levels √ó evidence = 130 lines of careful calibration

**What's Included:**
- Backend Engineer Assessment (6 dimensions, fully calibrated)
- 5 score levels per dimension with clear criteria
- Keywords and evidence examples
- Interview questions (easy/medium/hard)

**Trade-offs:**
- ‚úÖ **Pros**: High quality, battle-tested, complete documentation
- ‚ö†Ô∏è **Cons**: Only one role covered initially
- ‚ùå **Alternative Rejected**: 5+ incomplete templates - confusing, inconsistent

### Why Not More Templates?

**Considerations:**
1. **Calibration Time**: Each template requires 3-5 hours of careful tuning
2. **Role Expertise**: Backend Engineer is most common role, best understood
3. **Maintenance**: Fewer templates = easier to update framework changes
4. **Extensibility**: Template format is documented, users can create their own

---

## Alternative Approaches Considered

### 1. Voice-Only UI (No Text Interface)

**Rejected:** Pure voice conversation without text fallback.

**Why:**
- Accessibility: Some users prefer/require text
- Debugging: Text mode essential for development
- Reliability: Network issues affect audio quality
- Privacy: Some environments don't allow microphone

### 2. Real-Time GPT Streaming

**Rejected:** Stream GPT responses word-by-word during audio playback.

**Why:**
- Complexity: Requires splitting audio generation from text
- Latency: Overall slower (wait for first words, then TTS)
- Cost: No benefit for interview use case
- Reliability: More failure points in pipeline

### 3. Self-Hosted LLM

**Rejected:** Run Llama/Mistral locally instead of OpenAI API.

**Why:**
- Cost: GPU server ($500-2000/month) vs API ($0.001/interview)
- Quality: GPT-4o-mini more reliable for nuanced assessment
- Maintenance: Managing inference servers adds complexity
- Latency: Local models (even on GPU) slower than API

### 4. Video Interview

**Rejected:** Add video recording to capture body language.

**Why:**
- Storage: Video files 100x larger than audio
- Bandwidth: Requires high upload speeds
- Privacy: Many candidates uncomfortable with video
- Analysis: Video assessment beyond scope (would need CV models)
- Focus: Audio + text sufficient for technical assessment

### 5. Live Human-in-Loop

**Rejected:** Human interviewer reviews responses in real-time.

**Why:**
- Scalability: Defeats purpose of AI automation
- Cost: Human time expensive
- Consistency: Human bias reintroduced
- Latency: Slows interview flow

### 6. Blockchain for Verification

**Rejected:** Store assessments on blockchain for tamper-proof records.

**Why:**
- Overkill: PostgreSQL provides sufficient audit trail
- Cost: Gas fees add complexity
- Privacy: Candidate data shouldn't be public
- Maintenance: Blockchain adds operational complexity

---

## Performance Considerations

### Caching Strategy

**Decision:** Cache at multiple levels.

**Layers:**
1. **Redis**: Interview session state (TTL: 2 hours)
2. **PostgreSQL**: Historical data with indexes
3. **In-Memory**: Template parsing (loaded once at startup)

**Rationale:**
- **Speed**: Average response time <100ms (excluding LLM)
- **Cost**: Reduce database queries by 80%
- **UX**: Instant UI updates

### Async/Await Throughout

**Decision:** Use async/await for all I/O operations.

**Benefits:**
- Concurrent interviews: 100+ simultaneous users on single server
- Non-blocking: LLM calls don't block other requests
- Efficiency: Better resource utilization

---

## Security Decisions

### API Key Management

**Decision:** Use environment variables for secrets.

**Rationale:**
- **Standard**: Follows 12-factor app methodology
- **Security**: Keys never committed to Git
- **Flexibility**: Easy to rotate keys without code changes

### CORS Configuration

**Decision:** Restrictive CORS in production, permissive in development.

**Settings:**
```python
# Production
CORS_ORIGINS = ["https://yourdomain.com"]

# Development  
CORS_ORIGINS = ["*"]  # All origins
```

### Data Privacy

**Decision:** No PII logging, encrypted database connections.

**Measures:**
- Structured logging without sensitive data
- PostgreSQL SSL connections in production
- Audio files deleted after processing
- GDPR-compliant data retention

---

## Deployment Decisions

### Docker Containerization

**Decision:** Provide Docker Compose for easy deployment.

**Rationale:**
- **Consistency**: Same environment dev/staging/prod
- **Isolation**: Dependencies don't conflict with host
- **Portability**: Deploy to any Docker-capable platform

**Services:**
- Backend (FastAPI)
- Frontend (Streamlit √ó 2)
- PostgreSQL
- Redis

### Platform Agnostic

**Decision:** Support 5+ deployment platforms.

**Supported:**
1. Local (Docker Compose)
2. AWS Elastic Beanstalk
3. Google Cloud Run
4. Heroku
5. DigitalOcean App Platform
6. Generic VPS

**Why:** Maximize user choice, avoid vendor lock-in.

---

## Testing Strategy

### Manual Testing Focus

**Decision:** Prioritize integration testing over unit tests.

**Rationale:**
- **Time**: 2-week project timeline
- **Value**: End-to-end tests catch more issues
- **LLM**: Unit testing LLM responses non-deterministic
- **Coverage**: Key paths tested manually

**What's Tested:**
- Complete interview flow (end-to-end)
- Report generation with all sections
- Audio pipeline (all 3 TTS engines)
- WebSocket connectivity
- Database migrations

**Trade-offs:**
- ‚úÖ **Pros**: Fast development, real-world validation
- ‚ö†Ô∏è **Cons**: Less regression protection
- üîÑ **Future**: Add pytest suite for CI/CD

---

## Future Considerations

### What We'd Change With More Time

1. **React Frontend**: Replace Streamlit with proper SPA
2. **Video Support**: Add optional video recording
3. **Multi-Language**: Support non-English interviews
4. **Advanced Analytics**: Candidate comparison, trend analysis
5. **A/B Testing**: Experiment with different prompts/weights
6. **Mobile App**: Native iOS/Android clients
7. **Self-Hosted LLM**: Option for air-gapped deployments

### What We'd Keep

1. **6-Dimension Framework**: Well-calibrated, actionable
2. **Multi-Engine TTS**: Reliability worth the complexity
3. **PostgreSQL JSONB**: Perfect for flexible assessment data
4. **FastAPI**: Modern, fast, excellent DX
5. **Template System**: Extensible, version-controlled

---

## Conclusion

This platform represents a pragmatic balance between:
- **Quality** vs. Time-to-Market
- **Features** vs. Complexity
- **Cost** vs. Performance
- **Flexibility** vs. Opinionation

Every decision was made with the following priorities:
1. **Reliability**: System must work consistently
2. **User Experience**: Fast, intuitive, accessible
3. **Maintainability**: Code should be easy to understand/extend
4. **Cost-Effectiveness**: Minimize operational expenses
5. **Scalability**: Handle 100+ concurrent interviews

The result is a production-ready AI interviewer that balances technical sophistication with practical constraints.

---

**Document Version:** 1.0  
**Last Updated:** January 9, 2026  
**Author:** IOL AI Interviewer Team
